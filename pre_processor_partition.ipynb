{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a03d3b0-40c8-4969-98b4-84e593dd4930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Data Cleansing and Partitioning Pipeline ===\n",
      "\n",
      "[INFO] Reading file: ../data/raw_alerts.csv\n",
      "[INFO] Loaded 2718 rows with 99 columns\n",
      "[INFO] Selecting and type-correcting columns...\n",
      "[INFO] Columns removed: {'', 'pm_10_threshold', 'tamper', 'cell_phone', 'glass_threshold', 'wrapped_device', 'alpha_divisor', 'etoh_threshold', 'loitering_threshold', 'tamper_enabled', 'client_site_id', 'move_z', 'env', 'person_detection_threshold', 'risk', 'software_version', 'thc_level', 'inci_stddev_mult', 'device_type', 'client_org', 'co_threshold', 'client_name', 'audio_stream', 'tamper_polarity', 'gunshot_threshold', 'inci_back_time', 'co2_threshold', 'client_id', 'noise_ratio_threshold', 'keywords', 'client_reseller_id', 'crowd_count_thresh', 'move', 'vape_level', 'audio_server', 'aqi_threshold', 'tamper_debounce', 'temperature_f', 'crowd_count_dist', 'tvoc_threshold', 'audio_shift', 'move_x', 'cell_phone_threshold', 'hcho_threshold', 'scream_level', 'client_organization_id', 'crowd_count_dur', 'mid', 'inci_thresh_min', 'crowd_count_threshold', 'inci_curr_time', 'reseller_name', 'vape_min_interval', 'gun_level', 'organization_name', 'humidity_threshold', 'pm_25_threshold', 'thresh_min', 'tz', 'loud_time', 'site_name', 'ip', 'pm_1_threshold', 'stddev_mult', 'ehi_threshold', 'motion', 'ehi', 'no2_threshold', 'loud_thresh', 'move_y', 'glass_level', 'temperature_f_threshold', 'vape_alg', 'masking_level', 'client_site', 'temperature_threshold', 'audio_port'}\n",
      "[INFO] Columns kept: ['device_id', 'time', 'temperature', 'humidity', 'tvoc', 'co2', 'co', 'no2', 'hcho', 'pm_1', 'pm_25', 'pm_10', 'etoh', 'person_detection', 'loitering', 'aqi', 'noise_ratio', 'crowd_count', 'alert_type', 'alert_threshold', 'alert_value', 'device_name']\n",
      "[INFO] Rounding time to minute and aggregating to 10-minute intervals...\n",
      "[INFO] Aggregated to 1526 10-min intervals (from 2718 minute-level rows)\n",
      "[INFO] Handling missing/NaN values...\n",
      "[INFO] Partitioning data and writing output CSV files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "riting alerts: 100%|█████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 167.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Partitioned and wrote 40 files for alerts.\n",
      "[DONE] Processing complete for alerts (../data/raw_alerts.csv)\n",
      "\n",
      "\n",
      "[INFO] Reading file: ../data/raw_heartbeats.csv\n",
      "[INFO] Loaded 434311 rows with 92 columns\n",
      "[INFO] Selecting and type-correcting columns...\n",
      "[INFO] Columns removed: {'', 'pm_10_threshold', 'tamper', 'cell_phone', 'glass_threshold', 'wrapped_device', 'alpha_divisor', 'etoh_threshold', 'loitering_threshold', 'tamper_enabled', 'client_site_id', 'move_z', 'env', 'person_detection_threshold', 'risk', 'software_version', 'inci_stddev_mult', 'device_type', 'co_threshold', 'client_name', 'audio_stream', 'tamper_polarity', 'gunshot_threshold', 'inci_back_time', 'co2_threshold', 'client_id', 'noise_ratio_threshold', 'client_reseller_id', 'crowd_count_thresh', 'move', 'audio_server', 'aqi_threshold', 'tamper_debounce', 'temperature_f', 'crowd_count_dist', 'tvoc_threshold', 'audio_shift', 'move_x', 'cell_phone_threshold', 'hcho_threshold', 'client_organization_id', 'crowd_count_dur', 'mid', 'inci_thresh_min', 'crowd_count_threshold', 'inci_curr_time', 'reseller_name', 'vape_min_interval', 'organization_name', 'humidity_threshold', 'pm_25_threshold', 'thresh_min', 'tz', 'loud_time', 'site_name', 'ip', 'connected', 'pm_1_threshold', 'hb', 'stddev_mult', 'ehi_threshold', 'motion', 'ehi', 'no2_threshold', 'loud_thresh', 'move_y', 'temperature_f_threshold', 'vape_alg', 'temperature_threshold', 'audio_port'}\n",
      "[INFO] Columns kept: ['device_id', 'time', 'temperature', 'humidity', 'tvoc', 'co2', 'co', 'no2', 'hcho', 'pm_1', 'pm_25', 'pm_10', 'etoh', 'person_detection', 'loitering', 'aqi', 'noise_ratio', 'crowd_count', 'alert_type', 'alert_threshold', 'alert_value', 'device_name']\n",
      "[INFO] Rounding time to minute and aggregating to 10-minute intervals...\n",
      "[INFO] Aggregated to 50368 10-min intervals (from 433890 minute-level rows)\n",
      "[INFO] Handling missing/NaN values...\n",
      "[INFO] Partitioning data and writing output CSV files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing heartbeats: 100%|███████████████████████████████████████████████████████████| 373/373 [00:03<00:00, 113.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Partitioned and wrote 373 files for heartbeats.\n",
      "[DONE] Processing complete for heartbeats (../data/raw_heartbeats.csv)\n",
      "\n",
      "\n",
      "=== All tasks complete. Data is ready for S3 upload and AWS Glue cataloging. ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from scipy.stats import mode\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    print(\"tqdm not found. Please install it for progress bars: pip install tqdm\")\n",
    "    tqdm = lambda x, **kwargs: x\n",
    "\n",
    "# ======= CONFIGURATION ======= #\n",
    "COLUMNS = [\n",
    "    \"device_id\", \"time\", \"temperature\", \"humidity\", \"tvoc\", \"co2\", \"co\", \"no2\", \"hcho\",\n",
    "    \"pm_1\", \"pm_25\", \"pm_10\", \"etoh\", \"person_detection\", \"loitering\", \"aqi\", \"noise_ratio\",\n",
    "    \"crowd_count\", \"alert_type\", \"alert_threshold\", \"alert_value\", \"device_name\"\n",
    "]\n",
    "# List of columns that should always be mode-aggregated (not averaged)\n",
    "INT_MODE_COLS = {\n",
    "    \"person_detection\", \"loitering\", \"crowd_count\", \"alert_type\",\n",
    "    \"alert_threshold\", \"alert_value\", \"aqi\"\n",
    "}\n",
    "# Raw data from ULTRA and PRO sensors - through infuxDB\n",
    "RAW_FILES = [\"../data/raw_alerts.csv\", \"../data/raw_heartbeats.csv\"]\n",
    "DATASET_NAMES = [\"alerts\", \"heartbeats\"]\n",
    "OUTPUT_BASE = \"data/partitioned_data\"\n",
    "# ============================= #\n",
    "\n",
    "def read_custom_csv(file_path):\n",
    "    print(f\"\\n[INFO] Reading file: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    dtype_row = lines[1].strip().split(\",\")\n",
    "    header_row = lines[3].strip().split(\",\")\n",
    "    dtype_map = dict(zip(header_row, dtype_row))\n",
    "    df = pd.read_csv(file_path, skiprows=4, header=None, names=header_row, low_memory=False)\n",
    "    print(f\"[INFO] Loaded {len(df)} rows with {len(df.columns)} columns\")\n",
    "    return df, header_row, dtype_map\n",
    "\n",
    "def select_and_type_columns(df, dtype_map):\n",
    "    print(\"[INFO] Selecting and type-correcting columns...\")\n",
    "    initial_cols = set(df.columns)\n",
    "    keep = [col for col in COLUMNS if col in df.columns]\n",
    "    removed = initial_cols - set(keep)\n",
    "    df = df[keep]\n",
    "    # Cast types\n",
    "    for col in df.columns:\n",
    "        typ = dtype_map.get(col, \"str\").lower()\n",
    "        if typ.startswith(\"int\"):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
    "        elif typ.startswith(\"float\") or typ == \"double\":\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        elif typ in {\"str\", \"object\", \"string\"}:\n",
    "            df[col] = df[col].astype(str)\n",
    "        elif typ in {\"bool\", \"boolean\"}:\n",
    "            df[col] = df[col].astype('boolean')\n",
    "    print(f\"[INFO] Columns removed: {removed}\")\n",
    "    print(f\"[INFO] Columns kept: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "def aggregate_10min(df):\n",
    "    \"\"\"\n",
    "    1. Truncates all timestamps to minute resolution (removes seconds/ms)\n",
    "    2. Aggregates to 10-min intervals.\n",
    "    3. Integer-like columns are always reduced with mode, never mean.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Rounding time to minute and aggregating to 10-minute intervals...\")\n",
    "    df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "    df = df.dropna(subset=['time', 'device_id'])\n",
    "    # Remove seconds and milliseconds from time (floor to minute)\n",
    "    df['time'] = df['time'].dt.floor('min')\n",
    "    # Create 10-minute bucket for aggregation\n",
    "    df['bucket'] = df['time'].dt.floor('10T')\n",
    "    key_cols = ['device_id', 'bucket']\n",
    "    agg_dict = {}\n",
    "    for col in df.columns:\n",
    "        if col in ['device_id', 'time', 'bucket', 'date']:\n",
    "            continue\n",
    "        # If the column should be mode-aggregated (int/categorical), always use mode\n",
    "        if col in INT_MODE_COLS or pd.api.types.is_integer_dtype(df[col]):\n",
    "            agg_dict[col] = lambda x: mode(x.dropna(), keepdims=True)[0][0] if len(x.dropna()) else np.nan\n",
    "        # Numeric columns (floats): average\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            agg_dict[col] = 'mean'\n",
    "        else:\n",
    "            agg_dict[col] = lambda x: mode(x.dropna(), keepdims=True)[0][0] if len(x.dropna()) else ''\n",
    "    grouped = df.groupby(key_cols)\n",
    "    agg_df = grouped.agg(agg_dict).reset_index()\n",
    "    # The \"time\" of each record is the left edge of the 10-min bucket (also floored to minute)\n",
    "    agg_df['time'] = agg_df['bucket']\n",
    "    agg_df = agg_df.drop(columns=['bucket'])\n",
    "    print(f\"[INFO] Aggregated to {len(agg_df)} 10-min intervals (from {len(df)} minute-level rows)\")\n",
    "    return agg_df\n",
    "\n",
    "def fix_temporal_order(df):\n",
    "    df = df.sort_values(by=['device_id', 'time'])\n",
    "    return df\n",
    "\n",
    "def handle_missing(df):\n",
    "    print(\"[INFO] Handling missing/NaN values...\")\n",
    "    for col in df.columns:\n",
    "        if col in ['device_id', 'time', 'date']:\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[col] = df[col].interpolate().fillna(method='bfill').fillna(method='ffill')\n",
    "        else:\n",
    "            mode_val = df[col].mode(dropna=True)\n",
    "            if len(mode_val) > 0:\n",
    "                df[col] = df[col].fillna(mode_val[0])\n",
    "            else:\n",
    "                df[col] = df[col].fillna('')\n",
    "    return df\n",
    "\n",
    "def partition_and_write(df, dataset_name):\n",
    "    print(\"[INFO] Partitioning data and writing output CSV files...\")\n",
    "    df['date'] = df['time'].dt.date.astype(str)\n",
    "    grouped = df.groupby(['device_id', 'date'])\n",
    "    num_partitions = grouped.ngroups\n",
    "    with tqdm(total=num_partitions, desc=f\"Writing {dataset_name}\") as pbar:\n",
    "        for (device, date), group in grouped:\n",
    "            out_dir = os.path.join(OUTPUT_BASE, dataset_name, f\"device_id={device}\", f\"date={date}\")\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            out_df = group.drop(columns=['device_id', 'date'])\n",
    "            out_file = os.path.join(out_dir, f\"{dataset_name}.csv\")\n",
    "            out_df.to_csv(out_file, index=False)\n",
    "            pbar.update(1)\n",
    "    print(f\"[INFO] Partitioned and wrote {num_partitions} files for {dataset_name}.\")\n",
    "\n",
    "def process_file(file_path, dataset_name):\n",
    "    df, header, dtype_map = read_custom_csv(file_path)\n",
    "    df = select_and_type_columns(df, dtype_map)\n",
    "    df = aggregate_10min(df)\n",
    "    df = fix_temporal_order(df)\n",
    "    df = handle_missing(df)\n",
    "    partition_and_write(df, dataset_name)\n",
    "    print(f\"[DONE] Processing complete for {dataset_name} ({file_path})\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Starting Data Cleansing and Partitioning Pipeline ===\")\n",
    "    for f, ds in zip(RAW_FILES, DATASET_NAMES):\n",
    "        process_file(f, ds)\n",
    "    print(\"\\n=== All tasks complete. Data is ready for S3 upload and AWS Glue cataloging. ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13c8a1-c7f3-48a3-8676-fd2373bad6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
